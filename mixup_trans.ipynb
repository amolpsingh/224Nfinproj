{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2106f65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "\n",
    "class MixUpTransformer(torch.nn.Module):     \n",
    "    def __init__(self, drop_rate=0.2, freeze_bert=False): \n",
    "        super(MixUpTransformer, self).__init__() \n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.drop_rate=drop_rate\n",
    "        self.freeze_bert=freeze_bert\n",
    "    \n",
    "        if freeze_bert:\n",
    "            for p in self.bert.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.feedforward = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(drop_rate),\n",
    "            torch.nn.Linear(768, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        output = self.feedforward(bert_output[1])\n",
    "        return output\n",
    "\n",
    "\n",
    "def tokenize_data(df, tokenizer):\n",
    "    tokenized_df = tokenizer(df['text'].tolist(), \n",
    "                                add_special_tokens=True,\n",
    "                                truncation=True, \n",
    "                                padding=\"max_length\",\n",
    "                                return_attention_mask=True)\n",
    "\n",
    "    input_ids = np.array(tokenized_df['input_ids'])\n",
    "    attention_mask = np.array(tokenized_df['attention_mask'])\n",
    "    return input_ids, attention_mask, np.array(df['label'])\n",
    "\n",
    "\n",
    "def compute_mixup(a, b, l):\n",
    "    return l * np.array(a) + (1 - l) * np.array(b)\n",
    "\n",
    "\n",
    "def get_example_idx(labels, target):\n",
    "    idx = np.random.randint(len(labels))\n",
    "    while labels[idx] != target:\n",
    "        idx = np.random.randint(len(labels))\n",
    "    return idx\n",
    "\n",
    "\n",
    "def gen_mixed_indices(labels):\n",
    "    mixed = []\n",
    "    for i in range(len(labels)):\n",
    "        if i % 4 == 0:\n",
    "            # a = 0, b = 0\n",
    "            a = get_example_idx(labels, 0)\n",
    "            b = get_example_idx(labels, 0)\n",
    "        elif i % 4 == 1:\n",
    "            # a = 1, b = 0\n",
    "            a = get_example_idx(labels, 1)\n",
    "            b = get_example_idx(labels, 0)\n",
    "        elif i % 4 == 2:\n",
    "            # a = 0, b = 1\n",
    "            a = get_example_idx(labels, 0)\n",
    "            b = get_example_idx(labels, 1)\n",
    "        else:\n",
    "            # a = 1, b = 1\n",
    "            a = get_example_idx(labels, 1)\n",
    "            b = get_example_idx(labels, 1)\n",
    "        mixed.append((a, b))\n",
    "              \n",
    "    random.shuffle(mixed)\n",
    "    return mixed\n",
    "            \n",
    "        \n",
    "        \n",
    "from collections import defaultdict  \n",
    "    \n",
    "def mixup_aug(input_ids, att_masks, labels, lam, n=5):\n",
    "    mixed_input_ids = []\n",
    "    mixed_att_masks = []\n",
    "    mixed_labels = []\n",
    "    \n",
    "    for a, b in gen_mixed_indices(labels):      \n",
    "        new_input = list(map(int, compute_mixup(input_ids[a], input_ids[b], lam)))\n",
    "        new_att = att_masks[a] if sum(att_masks[a]) > sum(att_masks[b]) else att_masks[b]\n",
    "        new_label = float(compute_mixup(labels[a], labels[b], lam))\n",
    "\n",
    "        mixed_input_ids.append(new_input)\n",
    "        mixed_att_masks.append(new_att)\n",
    "        mixed_labels.append(new_label)\n",
    "        \n",
    "    return np.array(mixed_input_ids), np.array(mixed_att_masks), np.array(mixed_labels)\n",
    "\n",
    "\n",
    "def create_dataloaders(inputs, masks, labels, batch_size=16):\n",
    "    input_tensor = torch.tensor(inputs)\n",
    "    mask_tensor = torch.tensor(masks)\n",
    "    labels_tensor = torch.tensor(labels)\n",
    "    dataset = TensorDataset(input_tensor, mask_tensor, \n",
    "                            labels_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, \n",
    "                            shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, scheduler, loss_function, epochs,       \n",
    "          train_dataloader, device, clip_value=2, verbose=False):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(\"Epoch\", epoch)\n",
    "        print(\"-----\")\n",
    "        model.train()   \n",
    "        for step, batch in enumerate(train_dataloader): \n",
    "            batch_inputs, batch_masks, batch_labels = \\\n",
    "                               tuple(b.to(device) for b in batch)\n",
    "            model.zero_grad()\n",
    "            outputs = model(batch_inputs, batch_masks)    \n",
    "            loss = loss_function(outputs.squeeze().float(), \n",
    "                             batch_labels.squeeze().float())\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"epoch {step * 100 / len(train_dataloader)} % done\")\n",
    "        print(f\"curr train loss = {loss}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, dataloader, device):\n",
    "    model.eval()\n",
    "    output = []\n",
    "    for batch in dataloader:\n",
    "        batch_inputs, batch_masks, batch_labels = \\\n",
    "                                  tuple(b.to(device) for b in batch)\n",
    "        with torch.no_grad():\n",
    "            output += model(batch_inputs, \n",
    "                            batch_masks).view(1,-1).tolist()[0]\n",
    "    return output\n",
    "\n",
    "\n",
    "def evaluate(test_pred, test_labels):\n",
    "    test_labels = test_labels[:50]\n",
    "    predictions = np.array([round(x) for x in test_pred])\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(test_labels, predictions),\n",
    "        \"precision\": precision_score(test_labels, predictions),\n",
    "        'recall': recall_score(test_labels, predictions),\n",
    "        'f1': f1_score(test_labels, predictions)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f70ffb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "labeled = pd.read_csv('data/labeled_scouting.csv')\n",
    "test = pd.read_csv('data/augment/test.csv')\n",
    "train = pd.concat([labeled, test]).drop_duplicates(keep=False)\n",
    "\n",
    "# tokenize all text data using BERT tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_input_ids, train_attention_mask, train_labels = tokenize_data(train, tokenizer)\n",
    "test_input_ids, test_attention_mask, test_labels = tokenize_data(test, tokenizer)\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d28d5621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lam 0.1\n",
      "defaultdict(<class 'int'>, {(0, 0): 1556, (0, 1): 1555, (1, 1): 1555, (1, 0): 1556})\n",
      "[0.  0.9 1.  ... 1.  1.  1. ]\n",
      "lam 0.2\n",
      "defaultdict(<class 'int'>, {(1, 1): 1555, (0, 1): 1555, (0, 0): 1556, (1, 0): 1556})\n",
      "[1.  0.8 1.  ... 0.8 0.2 0.8]\n",
      "lam 0.3\n",
      "defaultdict(<class 'int'>, {(0, 0): 1556, (1, 0): 1556, (1, 1): 1555, (0, 1): 1555})\n",
      "[0.  0.3 1.  ... 0.7 1.  0.3]\n",
      "lam 0.4\n",
      "defaultdict(<class 'int'>, {(1, 1): 1555, (0, 0): 1556, (1, 0): 1556, (0, 1): 1555})\n",
      "[1.  0.  1.  ... 0.6 0.6 0.6]\n",
      "lam 0.5\n",
      "defaultdict(<class 'int'>, {(1, 1): 1555, (0, 0): 1556, (1, 0): 1556, (0, 1): 1555})\n",
      "[1.  0.  0.5 ... 0.5 0.  0.5]\n"
     ]
    }
   ],
   "source": [
    "for lam in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    print(\"lam\", lam)\n",
    "\n",
    "    # mix-up algorithm for training data\n",
    "    mixed_input_ids, mixed_att_masks, mixed_labels = mixup_aug(train_input_ids, train_attention_mask, train_labels, lam)\n",
    "    \n",
    "    print(mixed_labels)\n",
    "    continue\n",
    "    # load data into batches (size 16)\n",
    "    train_dataloader = create_dataloaders(mixed_input_ids, mixed_att_masks, mixed_labels)\n",
    "    test_dataloader = create_dataloaders(test_input_ids[:50], test_attention_mask[:50], test_labels[:50])\n",
    "    \n",
    "    # train model\n",
    "    model = MixUpTransformer()\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "    epochs = 3\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,       \n",
    "                    num_warmup_steps=0, num_training_steps=len(train_dataloader) * epochs)\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "\n",
    "    trained_model = train_model(model, optimizer, scheduler, loss_function, epochs, \n",
    "    train_dataloader, device, verbose=True)\n",
    "    \n",
    "    # evaluate\n",
    "    test_pred = predict(trained_model, test_dataloader, device)\n",
    "    print(evaluate(test_pred, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d12cce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
