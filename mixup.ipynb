{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a14343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt, style; style.use('fivethirtyeight')\n",
    "import seaborn as sns; sns.set(context='talk')\n",
    "\n",
    "from IPython.display import display, HTML, IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4209c59c",
   "metadata": {},
   "source": [
    "Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a776b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>50%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <td>4587.500000</td>\n",
       "      <td>2648.450018</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4587.5</td>\n",
       "      <td>9174.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>20.810431</td>\n",
       "      <td>2.307676</td>\n",
       "      <td>15.30</td>\n",
       "      <td>21.0</td>\n",
       "      <td>31.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>2016.715095</td>\n",
       "      <td>1.979656</td>\n",
       "      <td>2013.00</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>2019.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eta</th>\n",
       "      <td>2018.802799</td>\n",
       "      <td>2.422862</td>\n",
       "      <td>2013.00</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>2025.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arm</th>\n",
       "      <td>53.824127</td>\n",
       "      <td>6.897962</td>\n",
       "      <td>30.00</td>\n",
       "      <td>55.0</td>\n",
       "      <td>80.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Changeup</th>\n",
       "      <td>49.858290</td>\n",
       "      <td>5.364976</td>\n",
       "      <td>30.00</td>\n",
       "      <td>50.0</td>\n",
       "      <td>70.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Control</th>\n",
       "      <td>49.205290</td>\n",
       "      <td>5.059892</td>\n",
       "      <td>30.00</td>\n",
       "      <td>50.0</td>\n",
       "      <td>70.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Curveball</th>\n",
       "      <td>52.929583</td>\n",
       "      <td>5.675287</td>\n",
       "      <td>35.00</td>\n",
       "      <td>55.0</td>\n",
       "      <td>70.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cutter</th>\n",
       "      <td>52.475000</td>\n",
       "      <td>4.936723</td>\n",
       "      <td>40.00</td>\n",
       "      <td>50.0</td>\n",
       "      <td>70.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fastball</th>\n",
       "      <td>59.531873</td>\n",
       "      <td>6.665786</td>\n",
       "      <td>40.00</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Field</th>\n",
       "      <td>51.633871</td>\n",
       "      <td>5.503923</td>\n",
       "      <td>30.00</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hit</th>\n",
       "      <td>49.681105</td>\n",
       "      <td>5.510597</td>\n",
       "      <td>30.00</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power</th>\n",
       "      <td>47.932818</td>\n",
       "      <td>9.420963</td>\n",
       "      <td>20.00</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run</th>\n",
       "      <td>48.837240</td>\n",
       "      <td>12.169090</td>\n",
       "      <td>20.00</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Slider</th>\n",
       "      <td>52.735618</td>\n",
       "      <td>5.121491</td>\n",
       "      <td>30.00</td>\n",
       "      <td>50.0</td>\n",
       "      <td>70.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Splitter</th>\n",
       "      <td>53.333333</td>\n",
       "      <td>7.637626</td>\n",
       "      <td>40.00</td>\n",
       "      <td>50.0</td>\n",
       "      <td>70.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlb_played_first</th>\n",
       "      <td>2017.065463</td>\n",
       "      <td>1.735580</td>\n",
       "      <td>2010.00</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>2019.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>debut_age</th>\n",
       "      <td>23.374194</td>\n",
       "      <td>1.558851</td>\n",
       "      <td>18.89</td>\n",
       "      <td>23.4</td>\n",
       "      <td>29.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>0.078147</td>\n",
       "      <td>0.613681</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         mean          std      min     50%      max\n",
       "Unnamed: 0        4587.500000  2648.450018     1.00  4587.5  9174.00\n",
       "age                 20.810431     2.307676    15.30    21.0    31.90\n",
       "year              2016.715095     1.979656  2013.00  2017.0  2019.00\n",
       "eta               2018.802799     2.422862  2013.00  2019.0  2025.00\n",
       "Arm                 53.824127     6.897962    30.00    55.0    80.00\n",
       "Changeup            49.858290     5.364976    30.00    50.0    70.00\n",
       "Control             49.205290     5.059892    30.00    50.0    70.00\n",
       "Curveball           52.929583     5.675287    35.00    55.0    70.00\n",
       "Cutter              52.475000     4.936723    40.00    50.0    70.00\n",
       "Fastball            59.531873     6.665786    40.00    60.0    80.00\n",
       "Field               51.633871     5.503923    30.00    50.0    80.00\n",
       "Hit                 49.681105     5.510597    30.00    50.0    80.00\n",
       "Power               47.932818     9.420963    20.00    50.0    80.00\n",
       "Run                 48.837240    12.169090    20.00    50.0    80.00\n",
       "Slider              52.735618     5.121491    30.00    50.0    70.00\n",
       "Splitter            53.333333     7.637626    40.00    50.0    70.00\n",
       "mlb_played_first  2017.065463     1.735580  2010.00  2017.0  2019.00\n",
       "debut_age           23.374194     1.558851    18.89    23.4    29.27\n",
       "label                0.078147     0.613681    -1.00     0.0     1.00"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/twtc.csv')\n",
    "\n",
    "df_label_mask = (df.drop(columns='label') == 0).assign(label=False)\n",
    "df = df.mask(df_label_mask)\n",
    "\n",
    "df.describe().T[['mean', 'std', 'min', '50%', 'max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50c75d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (1.18.3)\n",
      "Requirement already satisfied: dill in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from datasets) (1.20.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from datasets) (2021.8.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: packaging in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from datasets) (21.0)\n",
      "Requirement already satisfied: xxhash in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: aiohttp in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from datasets) (7.0.0)\n",
      "Requirement already satisfied: pandas in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: multiprocess in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
      "Requirement already satisfied: filelock in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.3.1)\n",
      "Requirement already satisfied: pyyaml in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/eishmaheshwari/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38107a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertForSequenceClassification, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "379f1f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled = pd.read_csv('data/labeled_scouting.csv')\n",
    "test = pd.read_csv('data/augment/test.csv')\n",
    "train = pd.concat([labeled, test]).drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56def6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_corpus = tokenizer(text=train['text'].tolist(),\n",
    "#                             add_special_tokens=True,\n",
    "#                             padding='max_length',\n",
    "#                             truncation='longest_first',\n",
    "#                             max_length=300,\n",
    "#                             return_attention_mask=True)\n",
    "\n",
    "def tokenize_data(df, tokenizer):\n",
    "    tokenized_df = tokenizer(df['text'].tolist(), \n",
    "                                add_special_tokens=True,\n",
    "                                truncation=True, \n",
    "                                padding=\"max_length\",\n",
    "                                return_attention_mask=True)\n",
    "\n",
    "    input_ids = np.array(tokenized_df['input_ids'])\n",
    "    attention_mask = np.array(tokenized_df['attention_mask'])\n",
    "    return input_ids, attention_mask, np.array(df['label'])\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_input_ids, train_attention_mask, train_labels = tokenize_data(train, tokenizer)\n",
    "test_input_ids, test_attention_mask, test_labels = tokenize_data(test, tokenizer)\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "072c17c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON is a Level 1 sex offender and wouldn't be on our board were we running a draft room. The general consensus around the industry is that a team is going to draft PERSON, so we're compelled to cover his ability. We have him ranked where we think he fits on the continuum of talent, early in round two, though he probably won't be drafted until much later. PERSON was projected to go early in round two last year, when he was a junior. Shortly before the draft, The GPE reported court documents that showed PERSON plead guilty to sexually assaulting his niece. Court records showed the victim reported multiple incidents of molestation between 2009 and 2011, when PERSON was 14-15 years old and the victim was 4-6. He plead guilty to one count which included a handwritten admission and the the other count was dismissed as part of a plea bargain. After this information surface, PERSON spent the rest of the spring of 2017 away from GPE State and went undrafted. He returned for his senior season and has pitched well while, amid intermediate media attention, he and his family (except for the immediate family of the victim) denied he committed the crime and say PERSON plead guilty so the legal proceedings would end more quickly. This situation is abnormal, there's no precedent for it and it's unclear how a team would go about clearing PERSON for employment if they wanted to, though ownership would certainly have to be involved.\n",
      "[  101  2711  2003  1037  2504  1015  3348 25042  1998  2876  1005  1056\n",
      "  2022  2006  2256  2604  2020  2057  2770  1037  4433  2282  1012  1996\n",
      "  2236 10465  2105  1996  3068  2003  2008  1037  2136  2003  2183  2000\n",
      "  4433  2711  1010  2061  2057  1005  2128 15055  2000  3104  2010  3754\n",
      "  1012  2057  2031  2032  4396  2073  2057  2228  2002 16142  2006  1996\n",
      " 22961  1997  5848  1010  2220  1999  2461  2048  1010  2295  2002  2763\n",
      "  2180  1005  1056  2022  7462  2127  2172  2101  1012  2711  2001 11310\n",
      "  2000  2175  2220  1999  2461  2048  2197  2095  1010  2043  2002  2001\n",
      "  1037  3502  1012  3859  2077  1996  4433  1010  1996 14246  2063  2988\n",
      "  2457  5491  2008  3662  2711 25803  5905  2000 12581  6101  2075  2010\n",
      " 12286  1012  2457  2636  3662  1996  6778  2988  3674 10444  1997 16709\n",
      " 20100  2090  2268  1998  2249  1010  2043  2711  2001  2403  1011  2321\n",
      "  2086  2214  1998  1996  6778  2001  1018  1011  1020  1012  2002 25803\n",
      "  5905  2000  2028  4175  2029  2443  1037  2192 15773  9634  1998  1996\n",
      "  1996  2060  4175  2001  7219  2004  2112  1997  1037 14865 17113  1012\n",
      "  2044  2023  2592  3302  1010  2711  2985  1996  2717  1997  1996  3500\n",
      "  1997  2418  2185  2013 14246  2063  2110  1998  2253 21347  1012  2002\n",
      "  2513  2005  2010  3026  2161  1998  2038  8219  2092  2096  1010 13463\n",
      "  7783  2865  3086  1010  2002  1998  2010  2155  1006  3272  2005  1996\n",
      "  6234  2155  1997  1996  6778  1007  6380  2002  5462  1996  4126  1998\n",
      "  2360  2711 25803  5905  2061  1996  3423  8931  2052  2203  2062  2855\n",
      "  1012  2023  3663  2003 19470  1010  2045  1005  1055  2053 20056  2005\n",
      "  2009  1998  2009  1005  1055 10599  2129  1037  2136  2052  2175  2055\n",
      "  8430  2711  2005  6107  2065  2027  2359  2000  1010  2295  6095  2052\n",
      "  5121  2031  2000  2022  2920  1012   102     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(train['text'].iloc[0])\n",
    "\n",
    "print(train_input_ids[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bacac32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.70710678,  0.70710678, -1.22474487],\n",
       "       [ 1.41421356, -1.41421356,  0.        ],\n",
       "       [-0.70710678,  0.70710678,  1.22474487]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.random.randint(-1,3, size=(3, 3))\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ac27b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mixup(a, b, l):\n",
    "    return l * np.array(a) + (1 - l) * np.array(b)\n",
    "\n",
    "\n",
    "def mixup(row1, row2, l):\n",
    "    mixed_val = list(map(int, compute_mixup(row1['input_ids'].iloc[0], row2['input_ids'].iloc[0], l)))\n",
    "    mixed_label = compute_mixup(row1['label'], row2['label'], l)\n",
    "\n",
    "    if sum(row1['attention_mask'].iloc[0]) > sum(row2['attention_mask'].iloc[0]):\n",
    "        new_attention = row1['attention_mask'].iloc[0]\n",
    "    else:\n",
    "        new_attention = row2['attention_mask'].iloc[0]\n",
    "\n",
    "    return {\n",
    "        'label': mixed_label,\n",
    "        'input_ids': mixed_val,\n",
    "        'token_type_ids': row1['token_type_ids'].iloc[0],\n",
    "        'attention_mask': new_attention\n",
    "    }\n",
    "\n",
    "def mixup_aug(input_ids, att_masks, labels, lam, n=5):\n",
    "    mixed_input_ids = []\n",
    "    mixed_att_masks = []\n",
    "    mixed_labels = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        a = np.random.randint(len(input_ids))\n",
    "        b = np.random.randint(len(input_ids))\n",
    "        \n",
    "        x = list(map(int, compute_mixup(input_ids[a], input_ids[b], lam)))\n",
    "        print(x)\n",
    "        \n",
    "    \n",
    "        rows.append(mixup(row1, row2, lam))\n",
    "    tokenized_train = pd.DataFrame(rows)\n",
    "    shuffle(tokenized_train)\n",
    "    return Dataset.from_pandas(tokenized_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c18d047c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2465, 2078, 2603, 1406, 4222, 1010, 2403, 2167, 2192, 2187, 1616, 17840, 2037, 8931, 2172, 2556, 3986, 2534, 1422, 2061, 2601, 2035, 2171, 2951, 2182, 9378, 5133, 1517, 2341, 1673, 3033, 1010, 2203, 2005, 2707, 5175, 5256, 2090, 5759, 12313, 1607, 2102, 5309, 9893, 1601, 2232, 1416, 9345, 1406, 3842, 5262, 4701, 4159, 2053, 2047, 2070, 2211, 1606, 2011, 1643, 1670, 1429, 6326, 3602, 3301, 2393, 2065, 2199, 2221, 2007, 2618, 1705, 2056, 2454, 2292, 2089, 7359, 3180, 2026, 2429, 1405, 2674, 1998, 2202, 2250, 3833, 3575, 2411, 5378, 8931, 1421, 1404, 2432, 1606, 1436, 7176, 2321, 2951, 1627, 1612, 2261, 2165, 1027, 2519, 2909, 7473, 1980, 5092, 4392, 4776, 4181, 2038, 4780, 2323, 4236, 1850, 2410, 13567, 2774, 2862, 3167, 1416, 8010, 7018, 2059, 3156, 2050, 2413, 4565, 4433, 1401, 4664, 9086, 2473, 1612, 2051, 2114, 1613, 2013, 1404, 2352, 1420, 2352, 13936, 4213, 1998, 1420, 2703, 1603, 1423, 9789, 1021, 5122, 3115, 1611, 2017, 3887, 4323, 2002, 1461, 7028, 2008, 3791, 2379, 1562, 2012, 2003, 5945, 2030, 4091, 1641, 6395, 2344, 1614, 3020, 5524, 2581, 3575, 7137, 5327, 2029, 2085, 2389, 2615, 2005, 2037, 2523, 5448, 1999, 2013, 3424, 1682, 3709, 2924, 4132, 2255, 2010, 7589, 2226, 4341, 2100, 2001, 7439, 2784, 12844, 1418, 2012, 2649, 2999, 7472, 17946, 13430, 6032, 1614, 5440, 5910, 1997, 3013, 2770, 3213, 2045, 4162, 2262, 1758, 3362, 2559, 1404, 3160, 2890, 1913, 2321, 2325, 2052, 2426, 2059, 2885, 2505, 2011, 10056, 11918, 1988, 1298, 800, 1445, 953, 798, 1244, 800, 402, 422, 807, 1178, 1008, 404, 414, 976, 838, 799, 798, 1342, 1519, 838, 808, 414, 880, 881, 1292, 802, 798, 874, 404, 876, 848, 404, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'rows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wr/g3yq8sc12wq12qjr12b53r1w0000gn/T/ipykernel_32539/2400147213.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmixup_aug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/wr/g3yq8sc12wq12qjr12b53r1w0000gn/T/ipykernel_32539/1080038213.py\u001b[0m in \u001b[0;36mmixup_aug\u001b[0;34m(input_ids, att_masks, labels, lam, n)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mrows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mtokenized_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rows' is not defined"
     ]
    }
   ],
   "source": [
    "mixup_aug(train_input_ids, train_attention_mask, train_labels, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc32300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "def create_dataloaders(inputs, masks, labels, batch_size):\n",
    "    input_tensor = torch.tensor(inputs)\n",
    "    mask_tensor = torch.tensor(masks)\n",
    "    labels_tensor = torch.tensor(labels)\n",
    "    dataset = TensorDataset(input_tensor, mask_tensor, \n",
    "                            labels_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, \n",
    "                            shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "train_dataloader = create_dataloaders(train_input_ids, train_attention_mask, \n",
    "                                      train_labels, batch_size)\n",
    "test_dataloader = create_dataloaders(test_input_ids, test_attention_mask, \n",
    "                                     test_labels, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09123131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixup_aug(train, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7990c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "arguments = TrainingArguments(\n",
    "    output_dir=\"sample_SD_trainer\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\", # run validation at the end of each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=224\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Called at the end of validation. Gives accuracy\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # calculates the accuracy\n",
    "    return {\"accuracy\": np.mean(predictions == labels)}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=arguments,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test, # change to test when you do your final evaluation!\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10b45110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback, EarlyStoppingCallback\n",
    "\n",
    "class LoggingCallback(TrainerCallback):\n",
    "    def __init__(self, log_path):\n",
    "        self.log_path = log_path\n",
    "        \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        _ = logs.pop(\"total_flos\", None)\n",
    "        if state.is_local_process_zero:\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(json.dumps(logs) + \"\\n\")\n",
    "\n",
    "\n",
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=1, early_stopping_threshold=0.0))\n",
    "trainer.add_callback(LoggingCallback(\"sample_SD_trainer/log.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b29cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amolsingh/anaconda3/envs/local_nmt/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 6222\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1167\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='1167' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  19/1167 3:57:32 < 267:20:46, 0.00 it/s, Epoch 0.05/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c08c24",
   "metadata": {},
   "source": [
    "From_Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc27f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load our saved model, we can pass the path to the checkpoint into the `from_pretrained` method:\n",
    "test_str = \"I enjoyed the movie!\"\n",
    "\n",
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(\"sample_SD_trainer/checkpoint-24\")\n",
    "model_inputs = tokenizer(test_str, return_tensors=\"pt\")\n",
    "prediction = torch.argmax(finetuned_model(**model_inputs).logits)\n",
    "print([\"NEGATIVE\", \"POSITIVE\"][prediction])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
